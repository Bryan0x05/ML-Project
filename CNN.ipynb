{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools / Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for handling numbers\n",
    "import pandas as pd # for handling spreadsheet data\n",
    "import os # to retrive data-set files\n",
    "import cv2 # computer vision for extracting features from images\n",
    "import torch # pytorch for NN/CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[146, 166, 124], [137, 142, 80], [218, 189, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[237, 218, 227], [219, 205, 211], [209, 202, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[195, 174, 176], [193, 175, 176], [194, 176, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[74, 102, 133], [0, 23, 53], [3, 20, 47], [35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[181, 165, 152], [182, 166, 153], [183, 167, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[56, 37, 29], [48, 31, 22], [81, 63, 56], [17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[90, 92, 92], [86, 88, 88], [66, 67, 71], [86...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[160, 211, 207], [156, 207, 200], [178, 222, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[110, 98, 98], [112, 100, 100], [115, 102, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[[252, 252, 255], [250, 253, 255], [243, 251, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image label  \\\n",
       "0  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "1  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "2  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "3  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "4  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "5  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "6  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "7  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "8  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "9  c:\\Users\\bryan\\Documents\\Programming\\Git\\ML-Pr...  REAL   \n",
       "\n",
       "                                            features  \n",
       "0  [[146, 166, 124], [137, 142, 80], [218, 189, 8...  \n",
       "1  [[237, 218, 227], [219, 205, 211], [209, 202, ...  \n",
       "2  [[195, 174, 176], [193, 175, 176], [194, 176, ...  \n",
       "3  [[74, 102, 133], [0, 23, 53], [3, 20, 47], [35...  \n",
       "4  [[181, 165, 152], [182, 166, 153], [183, 167, ...  \n",
       "5  [[56, 37, 29], [48, 31, 22], [81, 63, 56], [17...  \n",
       "6  [[90, 92, 92], [86, 88, 88], [66, 67, 71], [86...  \n",
       "7  [[160, 211, 207], [156, 207, 200], [178, 222, ...  \n",
       "8  [[110, 98, 98], [112, 100, 100], [115, 102, 10...  \n",
       "9  [[252, 252, 255], [250, 253, 255], [243, 251, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to load dataset into a data frame\n",
    "# Encaplsated in a function to restrict the scope of variables that will not be needed in\n",
    "# later code blocks.\n",
    "# gray scale flag, by default it's off\n",
    "# ResizeX and ResizeY, fills these to resize image to desired size, if either\n",
    "# is 0 it's assumed to be off\n",
    "\n",
    "def loadData(grayscale:bool = False, resizeX: int = 0, resizeY = 0):\n",
    "    # helper function to fill the data variables with data from images\n",
    "    def fillData(data: dict,dir: str, label:str):\n",
    "        # list of all files in dir\n",
    "        # these values are the image files\n",
    "        list = os.listdir(dir)\n",
    "        # append image paths and labels in data dictionary\n",
    "        for image in list:\n",
    "            absImagePath = os.path.join(dir,image)\n",
    "            data['image'].append(absImagePath)\n",
    "            data['label'].append(label)\n",
    "            fileImage = cv2.imread(absImagePath)\n",
    "            # image gray scale\n",
    "            if(grayscale == True):\n",
    "                # 32 x 32, numpyarray\n",
    "                fileImage = cv2.cvtColor(fileImage,cv2.COLOR_BGR2GRAY)\n",
    "            if(resizeX > 0 and resizeY > 0):\n",
    "                target_size = (resizeX,resizeY)\n",
    "                fileImage = cv2.resize(fileImage,target_size)\n",
    "            # if it's color mode, reshape into 3-tuples(RGB)\n",
    "            if(grayscale == False):\n",
    "                data['features'].append(fileImage.reshape((-1,3)))\n",
    "            # else if it's gray scale, just flatten it\n",
    "            else: data['features'].append(fileImage.flatten())\n",
    "            \n",
    "    # dictionary to temporary house the data\n",
    "    # image = image path list, label = fake or real\n",
    "    trainData = {'image':[],'label':[], 'features':[]}\n",
    "    testData = {'image':[],'label':[], 'features':[]}\n",
    "   \n",
    "    # Read Train folder & Read test folder\n",
    "    # OS module used to ensure this works on all platforms that python runs on\n",
    "    currentDir = os.getcwd() # get's current directory to later append to image filepath for abs path\n",
    "    trainDirReal = os.path.join(currentDir,\"train\",'REAL') # abs file path to real class folder in training\n",
    "    trainDirFake = os.path.join(currentDir,'train','FAKE')\n",
    "    testDirReal = os.path.join(currentDir,\"test\",'REAL')\n",
    "    testDirFake = os.path.join(currentDir,\"test\",'FAKE')  \n",
    "\n",
    "\n",
    "    # helper function read file list from each folder and append abs path and labels\n",
    "    fillData(trainData,trainDirReal,'REAL')\n",
    "    fillData(trainData,trainDirFake,'FAKE')\n",
    "    fillData(testData,testDirReal,'REAL')\n",
    "    fillData(testData,testDirFake,'FAKE')\n",
    "        \n",
    "    # converts from dictionary type to dataframe for ease of access and compadability with\n",
    "    # ML library function calls\n",
    "    return pd.DataFrame(trainData), pd.DataFrame(testData)\n",
    "# executes function, returning 2 dataframes containing train and test data of both classes\n",
    "# Train and test data are seperated into different dataframes to enforce data hygiene \n",
    "trainData,testData = loadData(grayscale=False)\n",
    "# test if data was loaded successfully by outputing first 10 entries\n",
    "trainData.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # importing nueral networks from pytorch\n",
    "import torch.nn.functional as F # API for NN operations\n",
    "import torch.optim as optim # loss function and optimizer\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # 3 input(assumes color here), 6 output, kernel size of 5x5\n",
    "        self.pool = nn.MaxPool2d(2, 2) # max pooling layer with kernel 2 x 2 and stride of  2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input(from prior layer), 16 output channels and kernel 5 x 5\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # fully connected layers with input,output sizes\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2) # outputs to 2 neurons for 2 class problem\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "cnn = CNN()\n",
    "\n",
    "# loss function\n",
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "# opitimizer\n",
    "cnn_optimizer = optim.SGD(cnn.parameters(), lr = 0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataloader / Dataset Class For Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class customDataset(Dataset):\n",
    "    # init = class constructor\n",
    "    def __init__(self,dataframe=None, transform=None): \n",
    "        self.data = dataframe # designed to take in the pre-processed dataframe from code blocks above\n",
    "        self.transform = transform # sets the transform attribute of the class but doesn't apply it yet\n",
    "    # returns # of samples in dataset\n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "    # loads and returns a sample from the dataset at the given index 'idx'\n",
    "    def __getitem__(self,idx):\n",
    "        # grab row #idx\n",
    "        sample = self.data.iloc[idx]\n",
    "        # Grab data in column features of row #idx, this is image data in a flatten numpy array that contains full data of an image\n",
    "        image_data = sample['features']\n",
    "        # reshape the flattened array into 32x32x3 (height x width x color channels)\n",
    "        image_data = np.reshape(image_data,(32,32,3))\n",
    "        # convert to tensor\n",
    "        image_tensor = torch.from_numpy(image_data)\n",
    "        # apply transformation which is defined outside the function and passed whe __init__ is called(when the instance is constructed)\n",
    "        image_tensor = self.transform(image_data)\n",
    "        # get label strings\n",
    "        labels = sample['label']\n",
    "        # encode strings to numbers\n",
    "        if labels == 'FAKE':\n",
    "            labels = 0\n",
    "        else:\n",
    "            labels = 1\n",
    "        # return the image tensor and encoded labels        \n",
    "        return image_tensor, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NN\n",
    "Use the above class, to transform data into tensor format and dimensions to match pytorch expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size for training\n",
    "batchSize = 20\n",
    "# normalize images from [0,254] to [0,1], then to [-1,1] range tensor this is common practice for CNN\n",
    "# This reduces training time and increases rate of convergence\n",
    "# first parameter is mean, second std. The reason each is a 3 tuple, is it's per color(RGB)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# create custom pytorch dataset using the pandas pre-processed pandas dataframe\n",
    "traindataset = customDataset(dataframe=trainData, transform=transform)\n",
    "testdataset = customDataset(dataframe=testData,transform=transform)\n",
    "# create pytorch Dataloader for each custom dataset\n",
    "traindataloader = DataLoader(traindataset, batch_size=batchSize,shuffle=True)\n",
    "testdataloader = DataLoader(testdataset,batch_size=batchSize,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults to 5 iterations\n",
    "def trainNN(model,dataloader,iterations=5):\n",
    "    for epoch in range(iterations):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            # note: the dataloader will produce X instances and labels based on batch sized defined in the block above\n",
    "            inputs, labels = data\n",
    "        \n",
    "    \n",
    "            # zero the parameter gradients\n",
    "            cnn_optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = cnn_criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            cnn_optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainining the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# calls the above function to trai the CNN\n",
    "trainNN(cnn,traindataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 84 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85      5000\n",
      "           1       0.87      0.81      0.84      5000\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def testNNModel(NN, testloader):        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        # ensures expected behavior when evaluating the model by putting it into eval mode\n",
    "        NN.eval()\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = NN(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # update the true and predicted labels\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(predicted.numpy())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the {len(testData)} test images: {100 * correct // total} %')\n",
    "\n",
    "    # calculate the confusion matrix\n",
    "    print(classification_report(y_true, y_pred))\n",
    "# run the above function\n",
    "testNNModel(cnn,testdataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
